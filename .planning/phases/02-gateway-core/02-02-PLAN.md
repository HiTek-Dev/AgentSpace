---
phase: 02-gateway-core
plan: 02
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - packages/gateway/src/llm/types.ts
  - packages/gateway/src/llm/provider.ts
  - packages/gateway/src/llm/stream.ts
  - packages/gateway/src/llm/index.ts
  - packages/gateway/src/context/types.ts
  - packages/gateway/src/context/assembler.ts
  - packages/gateway/src/context/inspector.ts
  - packages/gateway/src/context/index.ts
  - packages/gateway/src/usage/pricing.ts
  - packages/gateway/src/usage/tracker.ts
  - packages/gateway/src/usage/store.ts
  - packages/gateway/src/usage/index.ts
  - packages/gateway/src/ws/handlers.ts
  - packages/gateway/src/ws/server.ts
  - packages/gateway/src/index.ts
autonomous: true

user_setup:
  - service: anthropic
    why: "LLM streaming requires an Anthropic API key"
    env_vars:
      - name: ANTHROPIC_API_KEY
        source: "Already stored in vault from Phase 1 onboarding. If not set, run: agentspace keys add anthropic"

must_haves:
  truths:
    - "User sends a chat message via WebSocket and receives a streaming response from Claude with text deltas arriving incrementally"
    - "After streaming completes, the user receives actual token usage and cost in the chat.stream.end message"
    - "User can request context inspection and sees byte count, token estimate, and cost estimate for each section (system_prompt, history, memory, skills, tools, user_message)"
    - "User can query usage totals and sees per-model breakdown with running totals"
    - "Gateway rejects a second chat.send while a stream is in progress on the same connection"
  artifacts:
    - path: "packages/gateway/src/llm/stream.ts"
      provides: "AI SDK 6 streamText wrapper with async generator yielding deltas and usage"
      contains: "streamText"
    - path: "packages/gateway/src/llm/provider.ts"
      provides: "Anthropic provider factory using vault API key"
      contains: "createAnthropic"
    - path: "packages/gateway/src/context/assembler.ts"
      provides: "Context assembly from session state with section measurement"
      contains: "assembleContext"
    - path: "packages/gateway/src/context/inspector.ts"
      provides: "Token estimation and cost calculation per section"
      contains: "inspectContext"
    - path: "packages/gateway/src/usage/tracker.ts"
      provides: "Per-request usage recording and aggregation queries"
      exports: ["UsageTracker"]
    - path: "packages/gateway/src/usage/pricing.ts"
      provides: "Model pricing table for Anthropic models"
      contains: "MODEL_PRICING"
    - path: "packages/gateway/src/ws/handlers.ts"
      provides: "Message handler implementations for chat.send, context.inspect, usage.query"
      exports: ["handleChatSend", "handleContextInspect", "handleUsageQuery"]
  key_links:
    - from: "packages/gateway/src/ws/handlers.ts"
      to: "packages/gateway/src/llm/stream.ts"
      via: "Calls streamChatResponse and iterates deltas to send WS messages"
      pattern: "streamChatResponse"
    - from: "packages/gateway/src/ws/handlers.ts"
      to: "packages/gateway/src/context/assembler.ts"
      via: "Assembles context before LLM call and for inspection"
      pattern: "assembleContext"
    - from: "packages/gateway/src/ws/handlers.ts"
      to: "packages/gateway/src/usage/tracker.ts"
      via: "Records usage after each completed stream"
      pattern: "usageTracker\\.record"
    - from: "packages/gateway/src/llm/provider.ts"
      to: "@agentspace/cli vault"
      via: "Retrieves Anthropic API key from vault"
      pattern: "getKey"
    - from: "packages/gateway/src/ws/server.ts"
      to: "packages/gateway/src/ws/handlers.ts"
      via: "Dispatches validated messages to handler functions"
      pattern: "handleChatSend|handleContextInspect|handleUsageQuery"
---

<objective>
Implement LLM streaming with AI SDK 6, context assembly with token/cost estimation, usage tracking with pricing, and wire all handlers into the WebSocket server -- replacing the NOT_IMPLEMENTED stubs from Plan 02-01.

Purpose: Deliver the core gateway functionality: users send messages, receive streaming responses, inspect context before sending, and track costs.
Output: Fully functional WebSocket gateway with streaming LLM responses, context inspection, and usage tracking.
</objective>

<execution_context>
@/Users/hitekmedia/.claude/get-shit-done/workflows/execute-plan.md
@/Users/hitekmedia/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-gateway-core/02-RESEARCH.md
@.planning/phases/02-gateway-core/02-01-SUMMARY.md
@packages/gateway/src/ws/protocol.ts
@packages/gateway/src/ws/server.ts
@packages/gateway/src/ws/connection.ts
@packages/gateway/src/session/manager.ts
@packages/gateway/src/session/store.ts
@packages/gateway/src/session/types.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: LLM provider, streaming, and usage/pricing modules</name>
  <files>
    packages/gateway/src/llm/types.ts
    packages/gateway/src/llm/provider.ts
    packages/gateway/src/llm/stream.ts
    packages/gateway/src/llm/index.ts
    packages/gateway/src/usage/pricing.ts
    packages/gateway/src/usage/store.ts
    packages/gateway/src/usage/tracker.ts
    packages/gateway/src/usage/index.ts
    packages/gateway/src/context/types.ts
    packages/gateway/src/context/assembler.ts
    packages/gateway/src/context/inspector.ts
    packages/gateway/src/context/index.ts
  </files>
  <action>
**1. Create `packages/gateway/src/llm/types.ts`:**
- Export `StreamDelta` type: `{ type: 'delta'; text: string }` and `StreamDone` type: `{ type: 'done'; usage: { inputTokens: number; outputTokens: number; totalTokens: number } }`
- Export `StreamChunk = StreamDelta | StreamDone` union type

**2. Create `packages/gateway/src/llm/provider.ts`:**
- `getAnthropicProvider()`: Retrieves Anthropic API key from vault using `getKey('anthropic')` from @agentspace/cli. If no key found, throw a descriptive error ("Anthropic API key not configured. Run: agentspace keys add anthropic"). Creates and returns `createAnthropic({ apiKey })` from @ai-sdk/anthropic.
- Cache the provider instance so it's not recreated on every request. Invalidate if key changes (simple: recreate on each call for now, optimize later).

**3. Create `packages/gateway/src/llm/stream.ts`:**
- `async function* streamChatResponse(model: string, messages: ModelMessage[], system?: string): AsyncGenerator<StreamChunk>` -- uses AI SDK 6's `streamText`:
  - Get provider via `getAnthropicProvider()`
  - Call `streamText({ model: provider(model), messages, system })`
  - `for await (const chunk of result.textStream)` yield `{ type: 'delta', text: chunk }`
  - After loop: `const usage = await result.usage`, yield `{ type: 'done', usage: { inputTokens: usage.inputTokens ?? 0, outputTokens: usage.outputTokens ?? 0, totalTokens: usage.totalTokens ?? (inputTokens + outputTokens) } }`
- **IMPORTANT:** Use `ModelMessage` type from `ai` (not `CoreMessage` which is AI SDK 5). Import `streamText` from `ai` and `createAnthropic` from `@ai-sdk/anthropic`.

**4. Create `packages/gateway/src/llm/index.ts`:** Barrel exports.

**5. Create `packages/gateway/src/usage/pricing.ts`:**
- Export `MODEL_PRICING` record mapping model strings to `{ inputPerMTok: number; outputPerMTok: number }`:
  - `'claude-opus-4.6'`: 5 / 25
  - `'claude-opus-4.5'`: 5 / 25
  - `'claude-sonnet-4.5'`: 3 / 15
  - `'claude-sonnet-4'`: 3 / 15
  - `'claude-haiku-4.5'`: 1 / 5
  - `'claude-haiku-3.5'`: 0.80 / 4
- Export `getModelPricing(model: string)`: returns pricing for model, falls back to `{ inputPerMTok: 3, outputPerMTok: 15 }` (Sonnet pricing) for unknown models.
- Export `calculateCost(model: string, inputTokens: number, outputTokens: number): { inputCost: number; outputCost: number; totalCost: number }`.

**6. Create `packages/gateway/src/usage/store.ts`:**
- `recordUsage(record: { sessionId, model, inputTokens, outputTokens, totalTokens, cost, timestamp }): void` -- insert into usage_records using Drizzle and getDb().
- `getUsageBySession(sessionId: string): UsageRow[]` -- select from usage_records where sessionId.
- `getUsageTotals(): { perModel: Record<string, { inputTokens, outputTokens, totalTokens, totalCost, requestCount }>, grandTotal: { totalCost, totalTokens, requestCount } }` -- aggregate query grouping by model. Use Drizzle's `sql` for SUM/COUNT aggregations.

All functions synchronous (better-sqlite3).

**7. Create `packages/gateway/src/usage/tracker.ts`:**
- `UsageTracker` class wrapping the store functions.
- `record(data)`: calls store.recordUsage, logs with createLogger.
- `querySession(sessionId)`: calls store.getUsageBySession.
- `queryTotals()`: calls store.getUsageTotals.
- Export singleton `usageTracker` instance.

**8. Create `packages/gateway/src/usage/index.ts`:** Barrel exports.

**9. Create `packages/gateway/src/context/types.ts`:**
- Export `ContextSection` interface: `{ name: string; content: string; byteCount: number; tokenEstimate: number; costEstimate: number }`
- Export `AssembledContext` interface: `{ sections: ContextSection[]; totals: { byteCount: number; tokenEstimate: number; costEstimate: number }; messages: ModelMessage[]; system?: string }`

**10. Create `packages/gateway/src/context/assembler.ts`:**
- `assembleContext(sessionMessages: MessageRow[], userMessage: string, model: string): AssembledContext`
  - Build sections array with `addSection()` helper:
    - `system_prompt`: Static "You are a helpful AI assistant." for Phase 2 (memory/soul come in Phase 5)
    - `history`: Format prior messages as `${role}: ${content}` joined by newlines. Use last 50 messages from sessionMessages.
    - `memory`: Empty string (stub for Phase 5)
    - `skills`: Empty string (stub for Phase 6)
    - `tools`: Empty string (stub for Phase 6)
    - `user_message`: The current user message content
  - For each section, compute byteCount via `Buffer.byteLength(content, 'utf8')`, tokenEstimate via `estimateTokenCount(content)` from tokenx (use 0 for empty strings), costEstimate as `(tokenEstimate / 1_000_000) * getModelPricing(model).inputPerMTok`
  - Build `messages: ModelMessage[]` array from session history + current user message for AI SDK
  - Set `system` to the system prompt string
  - Compute totals by summing sections
  - Return AssembledContext

**11. Create `packages/gateway/src/context/inspector.ts`:**
- `inspectContext(sessionMessages: MessageRow[], model: string): { sections: ContextSection[]; totals: ... }` -- same logic as assembleContext but WITHOUT the current user message (shows what would be sent next). Returns the sections and totals for the `context.inspection` WS response.

**12. Create `packages/gateway/src/context/index.ts`:** Barrel exports.
  </action>
  <verify>
Run `pnpm --filter @agentspace/gateway build` -- must succeed without type errors. Verify all 12 new files exist. Verify `MODEL_PRICING` has all 6 Anthropic models. Verify `streamChatResponse` uses `streamText` from `ai` (not direct Anthropic SDK).
  </verify>
  <done>LLM streaming module uses AI SDK 6 streamText with Anthropic provider sourcing keys from vault. Context assembler builds sections with byte/token/cost measurement using tokenx. Usage tracker records per-request data and computes per-model aggregations. All pricing for 6 Anthropic models configured.</done>
</task>

<task type="auto">
  <name>Task 2: Wire handlers into WebSocket server</name>
  <files>
    packages/gateway/src/ws/handlers.ts
    packages/gateway/src/ws/server.ts
    packages/gateway/src/index.ts
  </files>
  <action>
**1. Create `packages/gateway/src/ws/handlers.ts`** with three handler functions:

**`handleChatSend(socket, msg, connectionState)`:**
1. Check `connectionState.streaming` -- if true, send error `{ type: 'error', requestId: msg.id, code: 'STREAM_IN_PROGRESS', message: 'Please wait for the current response to complete' }` and return
2. Resolve or create session: if `msg.sessionId` provided, get from sessionManager. If not found, send error with code `SESSION_NOT_FOUND`. If no sessionId, create new session via sessionManager.create('default', msg.model ?? DEFAULT_MODEL), send `session.created` message.
3. Add user message to session: `sessionManager.addMessage(session.id, 'user', msg.content)`
4. Assemble context: `assembleContext(sessionManager.getMessages(session.id), msg.content, session.model)`
5. Set `connectionState.streaming = true`
6. Send `chat.stream.start` with requestId, sessionId, model
7. Try: iterate `streamChatResponse(session.model, context.messages, context.system)`:
   - On delta: send `chat.stream.delta` with requestId and delta text, accumulate fullResponse
   - On done: calculate cost via `calculateCost(session.model, usage.inputTokens, usage.outputTokens)`, record via `usageTracker.record(...)`, add assistant message `sessionManager.addMessage(session.id, 'assistant', fullResponse)`, send `chat.stream.end` with usage and cost
8. Catch errors: send `{ type: 'error', requestId, code: 'LLM_ERROR', message: error.message }`
9. Finally: set `connectionState.streaming = false`

**`handleContextInspect(socket, msg)`:**
1. Get session from sessionManager. If not found, send error `SESSION_NOT_FOUND`.
2. Call `inspectContext(sessionManager.getMessages(session.id), session.model)`
3. Send `context.inspection` response with sections and totals, requestId = msg.id

**`handleUsageQuery(socket, msg)`:**
1. If `msg.sessionId` provided, get session-specific usage via `usageTracker.querySession(msg.sessionId)` and format into per-model breakdown
2. If no sessionId, get global totals via `usageTracker.queryTotals()`
3. Send `usage.report` response with perModel and grandTotal, requestId = msg.id

**2. Update `packages/gateway/src/ws/server.ts`:**
- Import the three handlers from `./handlers.js`
- Replace the stub switch cases with actual handler calls
- Pass connectionState from the connection map to handleChatSend
- Log each message type at info level

**3. Update `packages/gateway/src/index.ts`:**
- Add exports for LLM, context, and usage modules
- Ensure the direct-run block still works with the full handler wiring
  </action>
  <verify>
Run `pnpm --filter @agentspace/gateway build` -- must succeed. Start the gateway: `pnpm --filter @agentspace/gateway exec tsx src/index.ts &`. Connect via wscat: `npx wscat -c ws://127.0.0.1:3271/gateway`. Send `{"type":"session.list","id":"s1"}` -- should return empty sessions list. Send `{"type":"chat.send","id":"c1","content":"Say hello in exactly 3 words"}` -- should return session.created, then chat.stream.start, multiple chat.stream.delta messages, then chat.stream.end with usage and cost. Send `{"type":"context.inspect","id":"i1","sessionId":"SESSION_ID_FROM_ABOVE"}` -- should return context.inspection with sections. Send `{"type":"usage.query","id":"u1"}` -- should return usage.report with totals.
  </verify>
  <done>All four WebSocket message types fully functional: chat.send streams responses from Claude via AI SDK 6, context.inspect returns section-by-section byte/token/cost breakdown, usage.query returns per-model totals, session.list returns all sessions. Concurrent stream guard rejects overlapping requests. Errors return structured error messages.</done>
</task>

</tasks>

<verification>
1. `pnpm build` succeeds across all packages
2. Gateway starts and accepts WebSocket connections at /gateway
3. Sending a chat message returns a streaming response from Claude (requires valid Anthropic API key in vault)
4. chat.stream.end includes inputTokens, outputTokens, totalTokens, inputCost, outputCost, totalCost
5. context.inspect returns sections with byte counts, token estimates, and cost estimates
6. usage.query returns per-model breakdown with running totals
7. Sending a second chat.send while streaming returns STREAM_IN_PROGRESS error
8. Invalid session IDs return SESSION_NOT_FOUND error
9. Missing API key returns NO_API_KEY or descriptive error
</verification>

<success_criteria>
- User sends a message via WebSocket and receives streaming text deltas followed by usage/cost summary
- Context inspector shows all sections (system_prompt, history, memory, skills, tools) with measurements
- Usage tracker records each request and provides per-model aggregation
- Error handling covers missing API key, invalid session, concurrent streams, and LLM errors
</success_criteria>

<output>
After completion, create `.planning/phases/02-gateway-core/02-02-SUMMARY.md`
</output>
