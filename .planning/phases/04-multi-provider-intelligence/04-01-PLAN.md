---
phase: 04-multi-provider-intelligence
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - packages/gateway/src/llm/registry.ts
  - packages/gateway/src/llm/provider.ts
  - packages/gateway/src/llm/stream.ts
  - packages/gateway/src/llm/types.ts
  - packages/gateway/src/llm/index.ts
  - packages/gateway/src/usage/pricing.ts
  - packages/gateway/src/ws/handlers.ts
  - packages/gateway/src/session/types.ts
  - packages/gateway/package.json
autonomous: true

must_haves:
  truths:
    - "User can send a chat.send with model 'anthropic:claude-sonnet-4-5' and receive a streaming response"
    - "User can send a chat.send with model 'openai:gpt-4o' and receive a streaming response (if OpenAI key configured)"
    - "Existing bare model names (e.g. 'claude-sonnet-4-5-20250929') still work with Anthropic as default provider"
    - "Usage tracking and pricing work correctly for Anthropic, OpenAI, and Ollama models"
  artifacts:
    - path: "packages/gateway/src/llm/registry.ts"
      provides: "Provider registry using AI SDK 6 createProviderRegistry"
      exports: ["getRegistry", "resolveModelId"]
    - path: "packages/gateway/src/llm/stream.ts"
      provides: "Unified streaming via registry instead of hardcoded Anthropic"
    - path: "packages/gateway/src/usage/pricing.ts"
      provides: "Extended pricing table with OpenAI models and Ollama wildcard"
  key_links:
    - from: "packages/gateway/src/llm/registry.ts"
      to: "@agentspace/cli/vault"
      via: "getKey() to source API keys for each provider"
      pattern: "getKey.*anthropic|openai"
    - from: "packages/gateway/src/llm/stream.ts"
      to: "packages/gateway/src/llm/registry.ts"
      via: "getRegistry().languageModel(modelId)"
      pattern: "registry\\.languageModel"
    - from: "packages/gateway/src/ws/handlers.ts"
      to: "packages/gateway/src/llm/registry.ts"
      via: "resolveModelId() for backward-compatible model resolution"
      pattern: "resolveModelId"
---

<objective>
Replace the single-provider Anthropic setup with a unified multi-provider registry supporting Anthropic, OpenAI, and Ollama through AI SDK 6's createProviderRegistry. Update streaming, pricing, and the chat handler to work with provider-qualified model IDs (e.g. "anthropic:claude-sonnet-4-5") while maintaining backward compatibility with existing bare model names.

Purpose: Enables SC-1 (send messages to any provider) and SC-2 (switch providers mid-conversation) at the infrastructure level.
Output: Provider registry module, updated streaming/pricing/handler code, new npm dependencies installed.
</objective>

<execution_context>
@/Users/hitekmedia/.claude/get-shit-done/workflows/execute-plan.md
@/Users/hitekmedia/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-multi-provider-intelligence/04-RESEARCH.md
@packages/gateway/src/llm/provider.ts
@packages/gateway/src/llm/stream.ts
@packages/gateway/src/llm/types.ts
@packages/gateway/src/llm/index.ts
@packages/gateway/src/usage/pricing.ts
@packages/gateway/src/ws/handlers.ts
@packages/gateway/src/ws/protocol.ts
@packages/gateway/src/session/types.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install provider packages and create provider registry</name>
  <files>
    packages/gateway/package.json
    packages/gateway/src/llm/registry.ts
    packages/gateway/src/llm/provider.ts
    packages/gateway/src/llm/types.ts
    packages/gateway/src/llm/index.ts
    packages/gateway/src/session/types.ts
  </files>
  <action>
    1. Install new dependencies:
       `pnpm --filter @agentspace/gateway add @ai-sdk/openai@^3 @ai-sdk/openai-compatible@^2`

    2. Create `packages/gateway/src/llm/registry.ts`:
       - Import createProviderRegistry from "ai", createAnthropic, createOpenAI, createOpenAICompatible
       - Import getKey from "@agentspace/cli/vault"
       - Export `buildRegistry(keys?: { anthropic?: string; openai?: string })` that:
         - If no keys arg, reads from vault via getKey("anthropic"), getKey("openai")
         - Conditionally registers anthropic provider (only if key exists)
         - Conditionally registers openai provider (only if key exists)
         - Always registers ollama via createOpenAICompatible({ name: "ollama", baseURL: "http://localhost:11434/v1" })
         - Returns createProviderRegistry(providers)
       - Export singleton `getRegistry()` that lazy-initializes and caches the registry
       - Export `resolveModelId(model: string): string` that:
         - If model contains ":", return as-is (already provider-qualified)
         - Otherwise, prefix with "anthropic:" (backward compatibility for bare model names like "claude-sonnet-4-5-20250929")
       - Export `getAvailableProviders(): string[]` that returns list of configured provider names (check vault keys + always include "ollama")

    3. Update `packages/gateway/src/llm/types.ts`:
       - Add `ProviderName = "anthropic" | "openai" | "ollama"` type
       - Add `ModelTier = "high" | "standard" | "budget"` type (for plan 04-02 forward compatibility)
       - Keep existing StreamDelta, StreamDone, StreamChunk unchanged

    4. Update `packages/gateway/src/llm/provider.ts`:
       - Keep getAnthropicProvider() for backward compat but mark as @deprecated with JSDoc comment pointing to registry.ts
       - No functional changes, just the deprecation annotation

    5. Update `packages/gateway/src/llm/index.ts`:
       - Add exports: getRegistry, resolveModelId, getAvailableProviders from registry.ts
       - Add type exports for ProviderName, ModelTier

    6. Update `packages/gateway/src/session/types.ts`:
       - Update DEFAULT_MODEL to "anthropic:claude-sonnet-4-5-20250929" (provider-qualified)
       - This is the single source of truth for the default model

    Key decisions:
    - Singleton registry pattern (consistent with SessionManager and UsageTracker patterns in codebase)
    - resolveModelId() handles backward compat — bare names get "anthropic:" prefix
    - Ollama always registered even without a key (it's local, keyless)
  </action>
  <verify>
    `cd packages/gateway && npx tsc --noEmit` compiles without errors.
    `pnpm --filter @agentspace/gateway exec -- node -e "import('@ai-sdk/openai').then(() => console.log('ok'))"` confirms package installed.
  </verify>
  <done>
    registry.ts exports buildRegistry, getRegistry, resolveModelId, getAvailableProviders. DEFAULT_MODEL is provider-qualified. TypeScript compiles cleanly.
  </done>
</task>

<task type="auto">
  <name>Task 2: Update streaming, pricing, and chat handler for multi-provider</name>
  <files>
    packages/gateway/src/llm/stream.ts
    packages/gateway/src/usage/pricing.ts
    packages/gateway/src/ws/handlers.ts
  </files>
  <action>
    1. Refactor `packages/gateway/src/llm/stream.ts`:
       - Remove import of getAnthropicProvider
       - Import getRegistry from "./registry.js"
       - Change streamChatResponse signature: keep same params (model, messages, system)
       - Inside: call `getRegistry().languageModel(model)` instead of `provider(model)`
       - The model param is now expected to be provider-qualified (e.g. "anthropic:claude-sonnet-4-5-20250929")
       - Everything else (streaming loop, usage extraction) stays the same — AI SDK normalizes across providers

    2. Extend `packages/gateway/src/usage/pricing.ts`:
       - Add OpenAI models to MODEL_PRICING (all with "openai:" prefix):
         - "openai:gpt-4o": { inputPerMTok: 2.5, outputPerMTok: 10 }
         - "openai:gpt-4o-mini": { inputPerMTok: 0.15, outputPerMTok: 0.6 }
         - "openai:o3": { inputPerMTok: 2, outputPerMTok: 8 }
         - "openai:o3-mini": { inputPerMTok: 1.1, outputPerMTok: 4.4 }
         - "openai:o4-mini": { inputPerMTok: 1.1, outputPerMTok: 4.4 }
         - "openai:gpt-4.1": { inputPerMTok: 2, outputPerMTok: 8 }
         - "openai:gpt-4.1-mini": { inputPerMTok: 0.4, outputPerMTok: 1.6 }
         - "openai:gpt-4.1-nano": { inputPerMTok: 0.1, outputPerMTok: 0.4 }
       - Add Ollama wildcard entry: "ollama:*": { inputPerMTok: 0, outputPerMTok: 0 }
       - Update existing Anthropic entries to include "anthropic:" prefix (keep old bare names too for backward compat)
       - Update getModelPricing() to:
         a. Try exact match
         b. Try fuzzy match (existing logic, extended for provider-prefixed names)
         c. If model starts with "ollama:", return { inputPerMTok: 0, outputPerMTok: 0 }
         d. Fall back to Sonnet pricing

    3. Update `packages/gateway/src/ws/handlers.ts` handleChatSend():
       - Import resolveModelId from "../llm/index.js"
       - After resolving the model from session or msg.model, call `model = resolveModelId(model)` to ensure provider-qualified
       - If msg.model is provided and differs from session model, update the session model (for mid-conversation model switching per GATE-04):
         ```
         if (msg.model) {
           const resolvedModel = resolveModelId(msg.model);
           if (resolvedModel !== model) {
             sessionManager.updateModel(sessionId, resolvedModel);
             model = resolvedModel;
           }
         }
         ```
       - This requires adding an `updateModel(sessionId, model)` method to sessionManager — add it to the session store (update the sessions table row)
       - The `streamChatResponse(model, ...)` call now passes a provider-qualified model string

    4. Add `updateModel` to session manager:
       - In packages/gateway/src/session/store.ts (or wherever sessions are stored), add:
         `updateModel(sessionId: string, model: string): void` that updates the model column for the given session
       - Export from session/index.ts

    Key decisions:
    - Pricing keeps both bare and prefixed Anthropic entries for maximum backward compat
    - Ollama pricing uses special "ollama:*" wildcard check (starts-with)
    - Model switching happens inline in handleChatSend when msg.model differs from session model
  </action>
  <verify>
    `cd packages/gateway && npx tsc --noEmit` compiles without errors.
    `pnpm turbo build --filter=@agentspace/gateway` builds successfully.
    Verify resolveModelId("claude-sonnet-4-5-20250929") returns "anthropic:claude-sonnet-4-5-20250929" by inspection.
    Verify resolveModelId("openai:gpt-4o") returns "openai:gpt-4o" by inspection.
  </verify>
  <done>
    streamChatResponse uses registry instead of hardcoded Anthropic. Pricing covers Anthropic, OpenAI, and Ollama. handleChatSend resolves provider-qualified model IDs and supports mid-conversation model switching. TypeScript compiles and builds.
  </done>
</task>

</tasks>

<verification>
1. `pnpm turbo build` — full monorepo build succeeds
2. `npx tsc --noEmit` in packages/gateway — no type errors
3. Code review: stream.ts no longer imports getAnthropicProvider, uses registry instead
4. Code review: handlers.ts calls resolveModelId() before passing model to streamChatResponse
5. Code review: pricing.ts has entries for anthropic:*, openai:*, and ollama:* models
6. Code review: DEFAULT_MODEL in session/types.ts is "anthropic:claude-sonnet-4-5-20250929"
</verification>

<success_criteria>
- Provider registry created with lazy singleton pattern, sourcing keys from vault
- Streaming uses unified registry.languageModel() call
- Bare model names resolve to anthropic:* via resolveModelId()
- Pricing extended for OpenAI and Ollama
- Mid-conversation model switching works via msg.model field
- Full build passes with no type errors
</success_criteria>

<output>
After completion, create `.planning/phases/04-multi-provider-intelligence/04-01-SUMMARY.md`
</output>
