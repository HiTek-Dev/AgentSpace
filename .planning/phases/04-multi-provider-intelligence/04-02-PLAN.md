---
phase: 04-multi-provider-intelligence
plan: 02
type: execute
wave: 2
depends_on: ["04-01"]
files_modified:
  - packages/gateway/src/llm/router.ts
  - packages/gateway/src/llm/router-rules.ts
  - packages/gateway/src/llm/types.ts
  - packages/gateway/src/llm/index.ts
  - packages/gateway/src/ws/protocol.ts
  - packages/gateway/src/ws/handlers.ts
autonomous: true

must_haves:
  truths:
    - "Gateway classifies incoming messages into high/standard/budget complexity tiers"
    - "In auto mode, routing decision is shown in chat.stream.start but does not block"
    - "In manual mode, gateway sends chat.route.propose and waits for chat.route.confirm before streaming"
    - "User can override the proposed model in chat.route.confirm with accept=false and an override model"
    - "Routing only considers providers that have API keys configured (or Ollama if reachable)"
  artifacts:
    - path: "packages/gateway/src/llm/router.ts"
      provides: "Complexity classifier and routing engine"
      exports: ["classifyComplexity", "routeMessage", "Router"]
    - path: "packages/gateway/src/llm/router-rules.ts"
      provides: "Default routing rules and tier-to-model mapping"
      exports: ["DEFAULT_RULES", "DEFAULT_TIERS"]
    - path: "packages/gateway/src/ws/protocol.ts"
      provides: "chat.route.propose and chat.route.confirm protocol messages"
  key_links:
    - from: "packages/gateway/src/llm/router.ts"
      to: "packages/gateway/src/llm/registry.ts"
      via: "getAvailableProviders() to filter routing to available providers only"
      pattern: "getAvailableProviders"
    - from: "packages/gateway/src/ws/handlers.ts"
      to: "packages/gateway/src/llm/router.ts"
      via: "routeMessage() called before streamChatResponse in handleChatSend"
      pattern: "routeMessage"
    - from: "packages/gateway/src/ws/handlers.ts"
      to: "packages/gateway/src/ws/protocol.ts"
      via: "chat.route.propose sent, chat.route.confirm received"
      pattern: "chat\\.route\\.(propose|confirm)"
---

<objective>
Add intelligent task routing that classifies message complexity and selects the appropriate model tier, with a WebSocket proposal/confirm protocol for user visibility and override control. Implement three routing modes: auto (silent routing, visible in stream.start), manual (propose and wait for confirm), and off (use session default).

Purpose: Delivers SC-3 (automatic complexity-based routing) and SC-4 (user can see and override routing decisions).
Output: Router module with rules, extended WS protocol, updated chat handler with routing flow.
</objective>

<execution_context>
@/Users/hitekmedia/.claude/get-shit-done/workflows/execute-plan.md
@/Users/hitekmedia/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-multi-provider-intelligence/04-RESEARCH.md
@.planning/phases/04-multi-provider-intelligence/04-01-SUMMARY.md
@packages/gateway/src/llm/registry.ts
@packages/gateway/src/llm/types.ts
@packages/gateway/src/ws/protocol.ts
@packages/gateway/src/ws/handlers.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create routing engine and default rules</name>
  <files>
    packages/gateway/src/llm/router-rules.ts
    packages/gateway/src/llm/router.ts
    packages/gateway/src/llm/types.ts
    packages/gateway/src/llm/index.ts
  </files>
  <action>
    1. Add routing types to `packages/gateway/src/llm/types.ts`:
       ```
       export type RoutingMode = "auto" | "manual" | "off";

       export interface RoutingDecision {
         tier: ModelTier;
         provider: string;
         model: string;
         reason: string;
         confidence: number; // 0-1
       }

       export interface RoutingRule {
         tier: ModelTier;
         match: (message: string, historyLength: number) => boolean;
         priority: number; // lower = higher priority
       }

       export interface TierConfig {
         high: string;      // provider-qualified model ID
         standard: string;
         budget: string;
       }
       ```

    2. Create `packages/gateway/src/llm/router-rules.ts`:
       - Export DEFAULT_RULES: RoutingRule[] with three rules:
         - **high** (priority 1): matches if message contains planning/analysis keywords (/\b(plan|architect|design|analyze|compare|evaluate|debug complex|refactor|explain in detail|write a comprehensive)\b/i) OR message length > 2000 chars OR history > 20 messages
         - **budget** (priority 3): matches if message contains simple patterns (/\b(hi|hello|hey|thanks|thank you|what is|define|translate|summarize briefly|yes|no|ok|sure)\b/i) AND message length < 200 chars AND history < 5 messages
         - **standard** (priority 2): always matches (default fallback)
       - Export DEFAULT_TIERS: TierConfig with:
         - high: "anthropic:claude-sonnet-4-5-20250929"
         - standard: "anthropic:claude-sonnet-4-5-20250929"
         - budget: "anthropic:claude-haiku-4-5-20250929" (if haiku is available, else same as standard)

    3. Create `packages/gateway/src/llm/router.ts`:
       - Import getAvailableProviders from "./registry.js"
       - Import DEFAULT_RULES, DEFAULT_TIERS from "./router-rules.js"
       - Import types

       - Export `classifyComplexity(message: string, historyLength: number, rules?: RoutingRule[]): { tier: ModelTier; confidence: number }`:
         - Sort rules by priority (ascending)
         - Iterate rules, first match wins
         - Return tier and confidence (1.0 for explicit keyword match, 0.7 for length-based, 0.5 for default fallback)

       - Export `routeMessage(message: string, historyLength: number, opts?: { tiers?: TierConfig; rules?: RoutingRule[] }): RoutingDecision`:
         - Call classifyComplexity to get tier
         - Look up tier in tiers config (DEFAULT_TIERS or opts.tiers)
         - Extract provider from the model string (split on ":")
         - Check if provider is available (via getAvailableProviders())
         - If provider unavailable, fall back: try other tiers, or use first available provider with standard tier model
         - Return RoutingDecision { tier, provider, model, reason, confidence }
         - reason should be human-readable: "Complex task detected (keyword: 'analyze')" or "Simple greeting" or "Default routing"

       - Export `getAlternatives(decision: RoutingDecision, tiers?: TierConfig): Array<{ provider: string; model: string; tier: ModelTier }>`:
         - Return the other tier options not selected (for the proposal message)

    4. Update `packages/gateway/src/llm/index.ts`:
       - Export classifyComplexity, routeMessage, getAlternatives from router.ts
       - Export DEFAULT_RULES, DEFAULT_TIERS from router-rules.ts
       - Export new types: RoutingMode, RoutingDecision, RoutingRule, TierConfig, ModelTier
  </action>
  <verify>
    `cd packages/gateway && npx tsc --noEmit` compiles without errors.
    Code review: classifyComplexity("plan the architecture for a new API", 3) returns tier "high".
    Code review: classifyComplexity("hi", 1) returns tier "budget".
    Code review: classifyComplexity("help me write a function", 5) returns tier "standard".
  </verify>
  <done>
    Router classifies messages into high/standard/budget tiers using configurable rules. routeMessage returns a full RoutingDecision with provider availability checks. All types exported.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add routing protocol messages and wire routing into chat handler</name>
  <files>
    packages/gateway/src/ws/protocol.ts
    packages/gateway/src/ws/handlers.ts
  </files>
  <action>
    1. Extend `packages/gateway/src/ws/protocol.ts`:

       **New client message — ChatRouteConfirmSchema:**
       ```
       const ChatRouteConfirmSchema = z.object({
         type: z.literal("chat.route.confirm"),
         id: z.string(),
         requestId: z.string(),  // matches the proposal's requestId
         accept: z.boolean(),
         override: z.object({
           provider: z.string(),
           model: z.string(),
         }).optional(),  // present when accept=false
       });
       ```
       - Add to ClientMessageSchema discriminated union
       - Export ChatRouteConfirm type

       **New server message — ChatRouteProposalSchema:**
       ```
       const ChatRouteProposalSchema = z.object({
         type: z.literal("chat.route.propose"),
         requestId: z.string(),
         sessionId: z.string(),
         routing: z.object({
           tier: z.enum(["high", "standard", "budget"]),
           provider: z.string(),
           model: z.string(),
           reason: z.string(),
           confidence: z.number(),
         }),
         alternatives: z.array(z.object({
           provider: z.string(),
           model: z.string(),
           tier: z.enum(["high", "standard", "budget"]),
         })),
       });
       ```
       - Add to ServerMessageSchema discriminated union
       - Export ChatRouteProposal type

       **Extend ChatStreamStartSchema:**
       - Add optional `routing` field: `routing: z.object({ tier: z.enum([...]), reason: z.string() }).optional()`
       - This carries routing info in auto mode so the client can display it

    2. Update `packages/gateway/src/ws/handlers.ts` handleChatSend():

       **Add routing mode support.** The routing mode is determined by:
       - If msg.model is explicitly set → skip routing (user chose the model)
       - Otherwise, use "auto" mode by default (hardcoded for now; will be configurable later via session/config)

       **Auto mode flow** (default):
       - After session resolution, before streaming:
         - Call `routeMessage(msg.content, sessionMessages.length)`
         - Use the routed model instead of session default
         - Include routing info in the chat.stream.start message: `routing: { tier, reason }`

       **Manual mode flow** (when routing mode is "manual"):
       - Call routeMessage()
       - Call getAlternatives()
       - Send chat.route.propose to client
       - Store pending routing state in connState (add pendingRouting field to ConnectionState type in connection.ts):
         `pendingRouting: { requestId: string; sessionId: string; content: string; messages: ModelMessage[] } | null`
       - Return (do not stream yet)

       **Add handleChatRouteConfirm handler:**
       - New exported function `handleChatRouteConfirm(socket, msg, connState)`
       - Retrieve pendingRouting from connState
       - If accept=true, use the originally proposed model
       - If accept=false and override provided, use override.provider + ":" + override.model (validate via resolveModelId)
       - Clear pendingRouting
       - Proceed with streaming (same logic as end of handleChatSend)
       - Extract the streaming portion of handleChatSend into a shared helper: `streamToClient(socket, model, sessionId, requestId, context, connState)`

       **Wire new handler in server.ts:**
       - Add case for "chat.route.confirm" in the WS message dispatcher (packages/gateway/src/ws/server.ts)
       - Import and call handleChatRouteConfirm

    3. Update ConnectionState in `packages/gateway/src/ws/connection.ts`:
       - Add `pendingRouting: { requestId: string; sessionId: string; content: string; routedModel: string } | null`
       - Initialize to null in connection factory

    Key decisions:
    - Default routing mode is "auto" (routing happens but doesn't block)
    - Manual mode requires explicit opt-in (future: via /routing slash command or session setting)
    - When user explicitly sets msg.model, routing is bypassed entirely
    - streamToClient helper avoids code duplication between normal flow and route-confirm flow
  </action>
  <verify>
    `cd packages/gateway && npx tsc --noEmit` compiles without errors.
    `pnpm turbo build` — full monorepo build succeeds.
    Code review: handleChatSend calls routeMessage when no explicit model is provided.
    Code review: chat.stream.start includes routing.tier and routing.reason in auto mode.
    Code review: chat.route.propose schema includes routing and alternatives fields.
    Code review: handleChatRouteConfirm properly retrieves pendingRouting and streams response.
  </verify>
  <done>
    Router integrated into chat flow. Auto mode routes silently with routing info in stream.start. Manual mode sends proposal and waits for confirm. User can override with alternative model. Protocol extended with chat.route.propose and chat.route.confirm messages.
  </done>
</task>

</tasks>

<verification>
1. `pnpm turbo build` — full monorepo build succeeds
2. `npx tsc --noEmit` in packages/gateway — no type errors
3. Code review: router.ts classifies "plan the architecture" as high tier
4. Code review: router.ts classifies "hi" as budget tier
5. Code review: protocol.ts has chat.route.propose in ServerMessageSchema and chat.route.confirm in ClientMessageSchema
6. Code review: handlers.ts routes in auto mode by default, bypasses routing when msg.model is explicit
7. Code review: handleChatRouteConfirm exists and handles accept/override
</verification>

<success_criteria>
- Complexity classifier categorizes messages into high/standard/budget tiers
- Auto routing mode includes tier and reason in chat.stream.start
- Manual routing mode sends proposal and waits for confirmation
- User can override proposed model via chat.route.confirm with accept=false
- Routing only selects from available providers (key configured or Ollama)
- Full build passes with no type errors
</success_criteria>

<output>
After completion, create `.planning/phases/04-multi-provider-intelligence/04-02-SUMMARY.md`
</output>
