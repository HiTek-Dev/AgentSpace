---
phase: 30-ollama-auto-discovery-and-remote-setup
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - packages/core/src/ollama/client.ts
  - packages/core/package.json
  - packages/cli/src/lib/models.ts
  - packages/cli/src/components/Onboarding.tsx
  - packages/cli/src/commands/init.ts
autonomous: true
requirements:
  - OLLM-01
  - OLLM-02
  - OLLM-03
  - OLLM-04

must_haves:
  truths:
    - "Running `tek init` on a machine with Ollama detects and lists available local models"
    - "Discovered Ollama models appear in the default model selection alongside cloud provider models"
    - "User can enter a remote Ollama IP:port during setup and see its available models"
    - "Remote Ollama endpoints are saved to ollamaEndpoints in config and registered in the gateway"
    - "Setup works gracefully when Ollama is not running (skip, no crash)"
  artifacts:
    - path: "packages/core/src/ollama/client.ts"
      provides: "Ollama discovery client with listOllamaModels and isOllamaReachable"
      exports: ["listOllamaModels", "isOllamaReachable", "OllamaModel", "OllamaTagsResponse"]
      min_lines: 50
    - path: "packages/cli/src/lib/models.ts"
      provides: "Async Ollama model catalog builder"
      exports: ["buildOllamaModelOptions"]
    - path: "packages/cli/src/components/Onboarding.tsx"
      provides: "Ollama auto-detect and remote endpoint onboarding steps"
      contains: "ollama-detect"
    - path: "packages/cli/src/commands/init.ts"
      provides: "ollamaEndpoints persisted to config from onboarding result"
      contains: "ollamaEndpoints"
  key_links:
    - from: "packages/cli/src/lib/models.ts"
      to: "packages/core/src/ollama/client.ts"
      via: "import { listOllamaModels }"
      pattern: "listOllamaModels"
    - from: "packages/cli/src/components/Onboarding.tsx"
      to: "packages/cli/src/lib/models.ts"
      via: "buildOllamaModelOptions call"
      pattern: "buildOllamaModelOptions"
    - from: "packages/cli/src/commands/init.ts"
      to: "config saveConfig"
      via: "ollamaEndpoints in AppConfig"
      pattern: "ollamaEndpoints"
---

<objective>
Implement Ollama auto-discovery and remote endpoint setup across core and CLI packages.

Purpose: Enable zero-config Ollama model detection during `tek init`, listing discovered models alongside cloud providers in the model selection step. Support manual remote Ollama endpoints with connectivity validation.

Output: Shared Ollama discovery client in @tek/core, async model catalog builder in CLI, and new onboarding steps for Ollama detection and remote endpoint entry.
</objective>

<execution_context>
@/Users/drew-mini/.claude/get-shit-done/workflows/execute-plan.md
@/Users/drew-mini/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/30-ollama-auto-discovery-and-remote-setup/30-RESEARCH.md
@packages/core/src/config/schema.ts
@packages/core/package.json
@packages/cli/src/lib/models.ts
@packages/cli/src/components/Onboarding.tsx
@packages/cli/src/commands/init.ts
@packages/gateway/src/llm/registry.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Ollama discovery client in @tek/core</name>
  <files>
    packages/core/src/ollama/client.ts
    packages/core/package.json
  </files>
  <action>
Create `packages/core/src/ollama/client.ts` with the following exports:

1. **`OllamaModel` interface** — matches the Ollama `/api/tags` response shape:
   - `name: string` (e.g. "llama3:latest")
   - `model: string`
   - `modified_at: string`
   - `size: number` (bytes on disk)
   - `digest: string`
   - `details: { format: string; family: string; families: string[]; parameter_size: string; quantization_level: string }`

2. **`OllamaTagsResponse` interface** — `{ models: OllamaModel[] }`

3. **`listOllamaModels(baseUrl = "http://localhost:11434", timeoutMs = 3000): Promise<OllamaModel[]>`**
   - Try native `GET {baseUrl}/api/tags` first (richer metadata: parameter_size, quantization_level, family)
   - Use `AbortController` with `timeoutMs` timeout to prevent hanging
   - On success, parse JSON and return `data.models`
   - On failure (network error, non-200, timeout), fall back to `GET {baseUrl}/v1/models` (OpenAI-compatible)
   - Convert OpenAI format `{ data: [{ id }] }` to `OllamaModel[]` shape with `"unknown"` for missing detail fields
   - On all failures, return empty array (never throw)

4. **`isOllamaReachable(baseUrl = "http://localhost:11434", timeoutMs = 2000): Promise<boolean>`**
   - `GET {baseUrl}/api/tags` with AbortController timeout
   - Return `true` if `res.ok`, `false` on any error
   - Never throw

Use Node.js built-in `fetch` (no new dependencies). Follow the exact implementation pattern from 30-RESEARCH.md Pattern 1.

Then update `packages/core/package.json` to add a new export path:
```json
"./ollama/client": {
  "import": "./dist/ollama/client.js",
  "types": "./dist/ollama/client.d.ts"
}
```

This does NOT need to be re-exported from the main core index.ts — it's a subpath export like `@tek/core/vault`.
  </action>
  <verify>
Run `cd packages/core && npx tsc --noEmit` to verify TypeScript compiles without errors. Verify the file exports the expected functions and types by checking the compiled output with `npx tsc && ls dist/ollama/`.
  </verify>
  <done>
`packages/core/src/ollama/client.ts` exists with `listOllamaModels`, `isOllamaReachable`, `OllamaModel`, and `OllamaTagsResponse` exports. `packages/core/package.json` has `./ollama/client` subpath export. TypeScript compiles cleanly.
  </done>
</task>

<task type="auto">
  <name>Task 2: Integrate Ollama discovery into CLI model catalog and onboarding</name>
  <files>
    packages/cli/src/lib/models.ts
    packages/cli/src/components/Onboarding.tsx
    packages/cli/src/commands/init.ts
  </files>
  <action>
**A. Update `packages/cli/src/lib/models.ts`:**

Add a new async function `buildOllamaModelOptions`:

```typescript
import { listOllamaModels, type OllamaModel } from "@tek/core/ollama/client";

export async function buildOllamaModelOptions(
  baseUrl = "http://localhost:11434",
): Promise<Array<{ label: string; value: string }>> {
  const models = await listOllamaModels(baseUrl);
  return models.map((m) => ({
    label: formatOllamaModelName(m),
    value: `ollama:${m.name}`,
  }));
}
```

Add a helper `formatOllamaModelName(m: OllamaModel): string` that displays:
- Model name
- Parameter size and quantization in parens if available (e.g. "(8B Q4_K_M)")
- Disk size in brackets if available (e.g. "[3.3GB]")
- Skip "unknown" values — only show fields with real data

Keep the existing synchronous `buildModelOptions` unchanged for cloud providers.

**B. Update `packages/cli/src/components/Onboarding.tsx`:**

1. Add new step types to `OnboardingStep` union:
   - `"ollama-detect"` — auto-probes localhost:11434 on mount
   - `"ollama-remote-ask"` — asks if user wants to add a remote Ollama
   - `"ollama-remote-input"` — text input for IP:port

2. Add state variables:
   - `ollamaEndpoints: Array<{ name: string; url: string }>` — accumulated endpoints
   - `ollamaModels: Array<{ label: string; value: string }>` — discovered models for display
   - `ollamaProbing: boolean` — loading state during probing
   - `ollamaLocalDetected: boolean` — whether localhost Ollama was found

3. Add `ollamaEndpoints` to `OnboardingResult` interface (optional field).

4. Implement the `ollama-detect` step:
   - Transition: from `brave-ask`/`brave-input` → `ollama-detect` (instead of going to `model-select`)
   - On mount, show "Detecting Ollama..." spinner text
   - Call `buildOllamaModelOptions("http://localhost:11434")`
   - If models found: show "Found Ollama with N models: [model list]" with options:
     - "Use these models" → save `{ name: "localhost", url: "http://localhost:11434" }` to ollamaEndpoints, move to `ollama-remote-ask`
     - "Skip Ollama" → move to `model-select`
   - If no models found: show "Ollama not detected on localhost" with options:
     - "Add remote Ollama" → move to `ollama-remote-input`
     - "Skip" → move to `model-select`
   - Use a `useEffect` hook for the probe, set results into state

5. Implement the `ollama-remote-ask` step:
   - "Add a remote Ollama instance? (e.g., 192.168.1.100:11434)"
   - `ConfirmInput` → `ollama-remote-input` on confirm, `model-select` on cancel

6. Implement the `ollama-remote-input` step:
   - `TextInput` for host:port (e.g., "192.168.1.100:11434")
   - On submit: construct URL as `http://{input}` (if no protocol given, prepend http://)
   - Probe with `buildOllamaModelOptions(url)`
   - If reachable: show models, save endpoint with name derived from host, move to `ollama-remote-ask` (allow adding more)
   - If not reachable: show error "Could not reach Ollama at {host}:{port}. Ensure the remote server has OLLAMA_HOST=0.0.0.0 configured." with options to retry or skip

7. Update `buildAvailableModels()` to include Ollama models:
   - Make it async or pre-populate `ollamaModels` state during the detect steps
   - When building model list for `model-select`, include the accumulated `ollamaModels` alongside cloud provider models

8. Update flow transitions:
   - After `brave-ask`/`brave-input` → go to `ollama-detect` instead of `model-select`
   - After all Ollama steps → go to `model-select`

9. Update summary step to show Ollama endpoint count if any configured.

**C. Update `packages/cli/src/commands/init.ts`:**

1. In the `onComplete` callback, read `result.ollamaEndpoints` and include it in the `config` object passed to `saveConfig`:
   ```typescript
   const config: AppConfig = {
     ...existingFields,
     ollamaEndpoints: result.ollamaEndpoints,
   };
   ```

This ensures remote endpoints are persisted to config and picked up by the gateway registry (which already reads `cfg.ollamaEndpoints`).

**Important implementation notes:**
- Do NOT store discovered model lists in config — only store endpoint URLs. Models are discovered dynamically.
- Use the model name from `/api/tags` as-is (e.g., `llama3:latest`). Don't strip tags.
- All discovery probes must use short timeouts (2-3 seconds) with AbortController to prevent UI freezing.
- Ollama detection is non-blocking — if Ollama isn't running, skip gracefully with clear messaging.
  </action>
  <verify>
Run `cd packages/cli && npx tsc --noEmit` to verify TypeScript compiles. Manually verify:
1. `Onboarding.tsx` has the new step types in the union
2. `OnboardingResult` includes optional `ollamaEndpoints`
3. `init.ts` passes `ollamaEndpoints` to `saveConfig`
4. `models.ts` has `buildOllamaModelOptions` exported
5. Full build: `npx turbo build` passes
  </verify>
  <done>
- `models.ts` exports `buildOllamaModelOptions` that probes Ollama and returns formatted model options
- `Onboarding.tsx` has `ollama-detect`, `ollama-remote-ask`, and `ollama-remote-input` steps that auto-detect local Ollama, list discovered models, and accept remote endpoint input
- `init.ts` persists `ollamaEndpoints` from the onboarding result into the app config
- Discovered Ollama models appear in the default model selection alongside cloud provider models
- Setup works gracefully when Ollama is not running (shows "not detected", allows skip)
- Remote endpoint validation shows helpful error when unreachable (OLLAMA_HOST hint)
- `npx turbo build` passes
  </done>
</task>

</tasks>

<verification>
1. `npx turbo build` completes without errors across all packages
2. `packages/core/dist/ollama/client.js` exists after build
3. `packages/cli/src/components/Onboarding.tsx` contains `ollama-detect` step
4. `packages/cli/src/lib/models.ts` exports `buildOllamaModelOptions`
5. `packages/cli/src/commands/init.ts` includes `ollamaEndpoints` in the saved config
6. `packages/core/package.json` has `./ollama/client` export path
7. TypeScript compiles cleanly with no type errors
</verification>

<success_criteria>
- Ollama discovery client is a reusable module in @tek/core accessible via `@tek/core/ollama/client`
- `tek init` flow includes Ollama auto-detection step that probes localhost:11434
- Discovered local Ollama models appear in the model selection dropdown
- Remote Ollama endpoint input is available with validation and helpful error messages
- ollamaEndpoints config is persisted, enabling the gateway registry to register them
- All existing onboarding functionality remains intact (no regressions)
- Graceful degradation when Ollama is not running
</success_criteria>

<output>
After completion, create `.planning/phases/30-ollama-auto-discovery-and-remote-setup/30-01-SUMMARY.md`
</output>
