---
phase: 32-structured-streaming-and-chat-formatting
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - packages/gateway/src/ws/protocol.ts
  - packages/gateway/src/llm/types.ts
  - packages/gateway/src/llm/stream.ts
  - packages/gateway/src/agent/tool-loop.ts
  - packages/gateway/src/context/assembler.ts
autonomous: true
requirements:
  - STRM-01
  - STRM-02
  - STRM-03
  - STRM-06

must_haves:
  truths:
    - "Gateway emits chat.stream.reasoning messages when provider returns reasoning content"
    - "Gateway emits chat.stream.source messages when provider returns source attributions"
    - "Both stream.ts and tool-loop.ts use fullStream and emit identical structured message types"
    - "Extended thinking is conditionally enabled for supported Anthropic models only"
    - "Every LLM call includes a response formatting system prompt section"
  artifacts:
    - path: "packages/gateway/src/ws/protocol.ts"
      provides: "ChatStreamReasoning, ChatStreamSource schemas and types"
      contains: "chat.stream.reasoning"
    - path: "packages/gateway/src/llm/types.ts"
      provides: "StreamReasoning and StreamSource chunk types"
      contains: "StreamReasoning"
    - path: "packages/gateway/src/llm/stream.ts"
      provides: "fullStream-based streaming with reasoning and source emission"
      contains: "fullStream"
    - path: "packages/gateway/src/agent/tool-loop.ts"
      provides: "reasoning and source event relay to transport"
      contains: "chat.stream.reasoning"
    - path: "packages/gateway/src/context/assembler.ts"
      provides: "Response formatting system prompt injection"
      contains: "Response Formatting"
  key_links:
    - from: "packages/gateway/src/llm/stream.ts"
      to: "packages/gateway/src/ws/protocol.ts"
      via: "transport.send with new message types"
      pattern: "chat\\.stream\\.reasoning"
    - from: "packages/gateway/src/agent/tool-loop.ts"
      to: "packages/gateway/src/ws/protocol.ts"
      via: "transport.send with new message types"
      pattern: "chat\\.stream\\.reasoning"
    - from: "packages/gateway/src/context/assembler.ts"
      to: "system prompt output"
      via: "RESPONSE_FORMAT_PROMPT appended to systemParts"
      pattern: "Response Formatting"
---

<objective>
Extend the gateway's streaming pipeline and WS protocol to emit structured content types (reasoning, sources) from AI SDK's fullStream, and inject a base response formatting system prompt into every LLM call.

Purpose: Transforms the gateway from flat text-only streaming into a structured protocol that carries typed content blocks, enabling clients to render reasoning blocks, source attributions, and well-formatted markdown.

Output: Updated protocol schemas, refactored streaming paths, and formatting system prompt injection.
</objective>

<execution_context>
@/Users/drew-mini/.claude/get-shit-done/workflows/execute-plan.md
@/Users/drew-mini/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/32-structured-streaming-and-chat-formatting/32-RESEARCH.md
@packages/gateway/src/ws/protocol.ts
@packages/gateway/src/llm/types.ts
@packages/gateway/src/llm/stream.ts
@packages/gateway/src/agent/tool-loop.ts
@packages/gateway/src/context/assembler.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Extend WS protocol and StreamChunk types with reasoning/source message types</name>
  <files>
    packages/gateway/src/ws/protocol.ts
    packages/gateway/src/llm/types.ts
  </files>
  <action>
**In `packages/gateway/src/ws/protocol.ts`:**

1. Add `ChatStreamReasoningSchema` after the existing `ChatStreamDeltaSchema`:
```typescript
const ChatStreamReasoningSchema = z.object({
  type: z.literal("chat.stream.reasoning"),
  requestId: z.string(),
  delta: z.string(),
});
```

2. Add `ChatStreamSourceSchema`:
```typescript
const ChatStreamSourceSchema = z.object({
  type: z.literal("chat.stream.source"),
  requestId: z.string(),
  source: z.object({
    url: z.string(),
    title: z.string().optional(),
  }),
});
```

3. Add optional `contentType` field to existing `ChatStreamDeltaSchema`:
```typescript
const ChatStreamDeltaSchema = z.object({
  type: z.literal("chat.stream.delta"),
  requestId: z.string(),
  delta: z.string(),
  contentType: z.enum(["text", "code"]).optional(),
});
```

4. Add both new schemas to the `ServerMessageSchema` discriminated union array (after `ChatStreamDeltaSchema`).

5. Export types for the new schemas:
```typescript
export type ChatStreamReasoning = z.infer<typeof ChatStreamReasoningSchema>;
export type ChatStreamSource = z.infer<typeof ChatStreamSourceSchema>;
```

**In `packages/gateway/src/llm/types.ts`:**

1. Add `StreamReasoning` interface:
```typescript
export interface StreamReasoning {
  type: "reasoning";
  text: string;
}
```

2. Add `StreamSource` interface:
```typescript
export interface StreamSource {
  type: "source";
  url: string;
  title?: string;
}
```

3. Update the `StreamChunk` union type to include the new types:
```typescript
export type StreamChunk = StreamDelta | StreamDone | StreamToolCall | StreamToolResult | StreamReasoning | StreamSource;
```
  </action>
  <verify>
Run `npx tsc --noEmit -p packages/gateway/tsconfig.json` to confirm no TypeScript errors. Verify the new schemas are in the ServerMessageSchema union by checking the file.
  </verify>
  <done>
protocol.ts has ChatStreamReasoning and ChatStreamSource schemas registered in ServerMessageSchema union. types.ts has StreamReasoning and StreamSource in StreamChunk union. ChatStreamDelta has optional contentType field. All types compile cleanly.
  </done>
</task>

<task type="auto">
  <name>Task 2: Refactor streaming paths to fullStream, add reasoning options, and inject formatting prompt</name>
  <files>
    packages/gateway/src/llm/stream.ts
    packages/gateway/src/agent/tool-loop.ts
    packages/gateway/src/context/assembler.ts
  </files>
  <action>
**In `packages/gateway/src/llm/stream.ts`:**

1. Replace `result.textStream` loop with `result.fullStream` iteration. Handle `text-delta`, `reasoning`, `source`, and `finish` part types:
```typescript
for await (const part of result.fullStream) {
  switch (part.type) {
    case "text-delta":
      yield { type: "delta", text: part.text };
      break;
    case "reasoning":
      yield { type: "reasoning", text: part.text };
      break;
    case "source":
      yield { type: "source", url: part.url, title: part.title };
      break;
    case "finish":
      break;
  }
}
```

2. Add a helper function `getReasoningOptions(model: string)` that returns `providerOptions` for Claude models supporting extended thinking:
```typescript
function getReasoningOptions(model: string): Record<string, unknown> | undefined {
  const isThinkingModel =
    model.includes("claude-opus-4") ||
    model.includes("claude-sonnet-4") ||
    model.includes("claude-3-7-sonnet");
  if (!isThinkingModel) return undefined;
  return {
    anthropic: {
      thinking: { type: "enabled", budgetTokens: 8000 },
    },
  };
}
```

3. Pass `providerOptions` to `streamText` when returned by `getReasoningOptions`:
```typescript
const providerOptions = getReasoningOptions(model);
const result = streamText({
  model: languageModel,
  messages,
  system,
  ...(providerOptions ? { providerOptions } : {}),
});
```

**In `packages/gateway/src/agent/tool-loop.ts`:**

1. Add `reasoning` case to the `fullStream` switch statement (after the `text-delta` case):
```typescript
case "reasoning": {
  transport.send({
    type: "chat.stream.reasoning",
    requestId,
    delta: part.text,
  });
  break;
}
```

2. Add `source` case:
```typescript
case "source": {
  transport.send({
    type: "chat.stream.source",
    requestId,
    source: { url: part.url, title: part.title },
  });
  break;
}
```

3. Import and use `getReasoningOptions` from `../llm/stream.js` (export the helper from stream.ts). Pass providerOptions to `streamText` in the agent loop:
```typescript
const providerOptions = getReasoningOptions(model);
const result = streamText({
  model: languageModel,
  messages,
  system,
  tools: tools as any,
  stopWhen: stepCountIs(maxSteps),
  ...(providerOptions ? { providerOptions } : {}),
  onStepFinish: async (stepResult) => { ... },
});
```

**In `packages/gateway/src/context/assembler.ts`:**

1. Add a `RESPONSE_FORMAT_PROMPT` constant at the module level (after `DEFAULT_SYSTEM_PROMPT`):
```typescript
const RESPONSE_FORMAT_PROMPT = [
  "",
  "## Response Formatting",
  "",
  "Format your responses for readability:",
  "- Use **markdown** for structured content: headings, lists, emphasis",
  "- Use fenced code blocks (```language) with the language identifier",
  "- Use inline `code` for names, paths, commands, and short references",
  "- Use numbered lists for sequential steps",
  "- Use > blockquotes for important notes or warnings",
  "- Keep responses conversational when a simple reply suffices",
  "- Do NOT wrap entire responses in a single code block",
].join("\n");
```

2. Append `RESPONSE_FORMAT_PROMPT` to the `systemParts` array in `assembleContext()`, after the `recentLogs` entry:
```typescript
const systemParts = [
  userSystemPrompt,
  // ... existing parts ...
  memoryCtx.recentLogs ? `\n\n# Recent Activity\n${memoryCtx.recentLogs}` : "",
  `\n\n${RESPONSE_FORMAT_PROMPT}`,
].filter(Boolean).join("");
```

3. Add a measured section for the formatting prompt:
```typescript
addSection(sections, "response_format", RESPONSE_FORMAT_PROMPT, pricing.inputPerMTok);
```
  </action>
  <verify>
Run `npx tsc --noEmit -p packages/gateway/tsconfig.json` to confirm no TypeScript errors. Verify stream.ts uses `fullStream` (not `textStream`). Verify tool-loop.ts handles `reasoning` and `source` cases. Verify assembler.ts includes `RESPONSE_FORMAT_PROMPT` in systemParts.
  </verify>
  <done>
stream.ts uses fullStream and yields reasoning/source chunks. tool-loop.ts relays reasoning and source events to transport. Both paths conditionally enable extended thinking for supported Anthropic models. assembler.ts injects a response formatting prompt section into every system prompt. TypeScript compiles cleanly.
  </done>
</task>

</tasks>

<verification>
1. `npx tsc --noEmit -p packages/gateway/tsconfig.json` passes with zero errors
2. protocol.ts ServerMessageSchema includes `ChatStreamReasoningSchema` and `ChatStreamSourceSchema`
3. stream.ts iterates `fullStream` (not `textStream`) and yields `reasoning` and `source` chunks
4. tool-loop.ts emits `chat.stream.reasoning` and `chat.stream.source` transport messages
5. assembler.ts system prompt output includes "## Response Formatting" section
6. ChatStreamDelta schema includes optional `contentType` field
</verification>

<success_criteria>
- Protocol extended with 2 new server message types (reasoning, source) without breaking existing messages
- Both streaming paths (stream.ts for simple chat, tool-loop.ts for agent loop) emit structured content types
- Extended thinking conditionally enabled for Claude models only
- Every assembled context includes a response formatting instruction section
- No new dependencies required
</success_criteria>

<output>
After completion, create `.planning/phases/32-structured-streaming-and-chat-formatting/32-01-SUMMARY.md`
</output>
